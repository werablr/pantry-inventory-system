ChatGPT round 2:
Got it — crystal clear now:

🔥 Updated Mission:

We are using LangChain. But we are not throwing away the months of work you already built. We’re wrapping your existing system with LangChain—not rewriting it.

You’re building on top of:
	•	Your working iPhone scanner app ✅
	•	Your existing Supabase schema ✅
	•	Your deployed Claude OCR pipeline ✅
	•	Your Nutritionix integration ✅
	•	Your legacy Edge Functions + real scan data ✅

⸻

✅ What This Means Technically

We are not replacing your code. We are:
	•	Keeping your existing Supabase as the system of record ✅
	•	Keeping your scanner app logic and UI ✅
	•	Keeping Claude in the loop ✅
	•	Adding LangChain as a wrapper to help with fallback, retries, routing, agent handoff ✅

LangChain becomes a middleware layer that:
	•	Observes your data stream
	•	Detects failures (e.g., missing expiration dates)
	•	Injects ChatGPT or another agent only when needed
	•	Logs decisions and outcomes
	•	Triggers backups or manual flows

⸻

🔄 So Let’s Reset and Align:

✔️ We Will:
	•	Keep everything you’ve already built
	•	Plug LangChain into the existing Claude pipeline
	•	Add fallback processing via ChatGPT
	•	Automate logging and resolution flows
	•	Use scaffolding only to help LangChain know what’s where

❌ We Will Not:
	•	Start over
	•	Rewrite your edge functions
	•	Replace the Supabase schema
	•	Rebuild the frontend

⸻

🚀 What We Do Next

Let me now deliver the LangChain script that:
	•	Connects to your existing Supabase
	•	Watches for rows where Claude failed (missing expiration date)
	•	Injects ChatGPT for fallback cleanup
	•	Logs resolution and updates Supabase

No code will be overwritten. No data is lost. All your architecture stays intact.

⸻

✅ What I Need to Build It

To finish the correct LangChain script (that fits into your system), I just need:
	1.	Supabase table name
→ e.g., scanned_items, scans, edge_data, etc.
	2.	OCR-related column names
	•	Raw OCR text field (e.g., ocr_text)
	•	Final expiration date field (e.g., expiration_date)
	•	(Optional) Claude output field if different
	3.	Supabase URL (safe to share)
→ https://xyzcompany.supabase.co

I’ll use placeholders for the secret key.

Once I have those, I’ll give you:
	•	A working LangChain-powered script (Python)
	•	That plugs directly into what you’ve already built

You built this thing. LangChain will now just support it.

Claude round 2:

# 🔗 LangChain Integration Plan - Scanner System v1.0

**Project:** Momma B's Nutrition Scanner - LangChain Enhancement  
**Owner:** Brian Rogers  
**Integration Strategy:** Wrapper Layer (preserve existing system)  
**Date:** 2025-07-04  

---

## 🎯 Integration Philosophy

**✅ KEEP (Working Components):**
- 43 Supabase tables with relationships
- 13 Edge Functions (especially `two-step-scanner v5`)
- Native iPhone app (deployed via EAS)
- Nutritionix API integration
- ML Kit OCR with confidence scoring

**🔗 ADD (LangChain Layer):**
- Intelligent OCR fallback when confidence < 0.8
- Multi-agent orchestration for complex decisions
- Memory and context for learning from corrections
- Automated retry logic and error recovery

---

## 📊 Current System Analysis

Based on your Supabase data:
```sql
-- Current OCR Performance (from my analysis)
Total scans (7 days): 23
Average OCR confidence: 71.5%
Scans with OCR data: 7
Scans needing review: 6
```

**Problem:** 71.5% average confidence is below target (≥85%)  
**Solution:** LangChain agent to enhance low-confidence OCR results

---

## 🏗️ Integration Architecture

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   iPhone App    │    │  LangChain      │    │   Supabase      │
│                 │    │  Orchestrator   │    │   Database      │
├─────────────────┤    ├─────────────────┤    ├─────────────────┤
│ • Barcode scan  │───▶│ • OCR enhance   │───▶│ • scanned_items │
│ • OCR capture   │    │ • Agent routing │    │ • products      │
│ • Manual entry  │    │ • Fallback logic│    │ • inventory     │
│ • Review UI     │    │ • Memory/learn  │    │ • storage_locs  │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                              ▲
                              │
                    ┌─────────────────┐
                    │  Edge Functions │
                    │                 │
                    │ • two-step-scan │
                    │ • review-api    │
                    │ • nutritionix   │
                    └─────────────────┘
```

**Key Insight:** LangChain sits **between** your Edge Functions and database, enhancing results before final storage.

---

## 🔧 Phase 1: Foundation Setup

### Step 1: Environment Setup (30 minutes)
```bash
# Create LangChain directory
mkdir langchain_scanner
cd langchain_scanner

# Install dependencies
pip install langchain openai supabase aiohttp python-dotenv

# Set environment variables
cat > .env << EOF
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your-service-role-key
SUPABASE_ANON_KEY=your-anon-key
OPENAI_API_KEY=your-openai-key
NUTRITIONIX_APP_ID=f4d58212
NUTRITIONIX_API_KEY=your-nutritionix-key
EOF
```

### Step 2: Core LangChain Script (Based on Your Schema)
```python
# scanner_orchestrator.py
import os
from langchain.chat_models import ChatOpenAI
from langchain.agents import initialize_agent, Tool
from supabase import create_client

class ScannerOrchestrator:
    def __init__(self):
        # Your actual Supabase connection
        self.supabase = create_client(
            os.getenv('SUPABASE_URL'),
            os.getenv('SUPABASE_SERVICE_KEY')
        )
        
        # GPT-4 for intelligent OCR enhancement
        self.llm = ChatOpenAI(
            model_name="gpt-4",
            temperature=0.1,
            openai_api_key=os.getenv('OPENAI_API_KEY')
        )
    
    def get_pending_scans(self):
        """Get scans from your actual scanned_items table"""
        return self.supabase.table('scanned_items').select(
            'id, barcode, ocr_text, extracted_expiry_date, '
            'ocr_confidence, verification_status, '
            'products(name, brand_name)'
        ).eq('verification_status', 'needs_review').execute()
    
    def enhance_ocr_with_gpt4(self, ocr_text, product_name):
        """Use GPT-4 to extract dates from failed OCR"""
        prompt = f"""
        Extract the expiration date from this OCR text from {product_name} packaging:
        
        OCR Text: "{ocr_text}"
        
        Return JSON: {{"extracted_date": "YYYY-MM-DD or null", "confidence": 0.0-1.0}}
        """
        
        response = self.llm.predict(prompt)
        # Parse and return enhancement
        return self.parse_gpt_response(response)
```

### Step 3: Integration Points with Your System
```python
# Integration with your existing Edge Functions
async def call_existing_edge_function(self, function_name, payload):
    """Call your existing two-step-scanner or review-api"""
    url = f"{self.supabase_url}/functions/v1/{function_name}"
    headers = {
        'Authorization': f'Bearer {self.anon_key}',
        'Content-Type': 'application/json'
    }
    
    async with aiohttp.ClientSession() as session:
        async with session.post(url, json=payload, headers=headers) as response:
            return await response.json()

# Update your actual database schema
def update_scan_with_enhancement(self, scan_id, extracted_date, confidence):
    """Update your scanned_items table with LangChain results"""
    return self.supabase.table('scanned_items').update({
        'extracted_expiry_date': extracted_date,
        'ocr_confidence': confidence,
        'date_extraction_method': 'langchain_enhanced',
        'verification_status': 'approved' if confidence > 0.8 else 'needs_review'
    }).eq('id', scan_id).execute()
```

---

## 📋 Phase 1 Implementation Checklist

### Week 1: Core Integration
- [ ] **Day 1**: Set up LangChain environment and dependencies
- [ ] **Day 2**: Create basic orchestrator that reads from `scanned_items`
- [ ] **Day 3**: Implement GPT-4 OCR enhancement tool
- [ ] **Day 4**: Test with 3 real scans from your database
- [ ] **Day 5**: Validate database updates work correctly

### Week 2: Agent Intelligence
- [ ] **Day 1**: Add intelligent routing (when to enhance vs skip)
- [ ] **Day 2**: Implement memory for learning from corrections
- [ ] **Day 3**: Add error handling and retry logic
- [ ] **Day 4**: Integration with your existing Edge Functions
- [ ] **Day 5**: Performance testing and optimization

### Week 3: Production Integration
- [ ] **Day 1**: Schedule orchestrator to run every 30 minutes
- [ ] **Day 2**: Monitor impact on OCR accuracy metrics
- [ ] **Day 3**: Fine-tune agent prompts based on results
- [ ] **Day 4**: Add logging and alerting
- [ ] **Day 5**: Documentation and handoff

---

## 🎯 Success Metrics

**Before LangChain:**
- Average OCR confidence: 71.5%
- Manual review required: 6/23 scans (26%)

**Target with LangChain:**
- Average OCR confidence: ≥85%
- Manual review required: <10% of scans
- Processing time: <30 seconds per enhancement

---

## 💡 Integration Strategy Details

### 1. **Minimal Disruption Approach**
```python
# Example: LangChain runs as background processor
def main_processing_loop():
    while True:
        # Get scans that need enhancement
        pending = get_scans_with_low_confidence()
        
        for scan in pending:
            if scan['ocr_confidence'] < 0.8:
                enhanced = enhance_with_langchain(scan)
                update_database(scan['id'], enhanced)
        
        time.sleep(1800)  # Run every 30 minutes
```

### 2. **Preserve Existing Workflows**
- Your iPhone app continues to use existing Edge Functions
- LangChain enhances results **after** initial processing
- Manual review interface remains unchanged
- All existing data relationships preserved

### 3. **Intelligent Enhancement Logic**
```python
def should_enhance_scan(scan):
    """Decide if LangChain should process this scan"""
    return (
        scan['ocr_confidence'] < 0.8 and  # Low confidence
        scan['ocr_text'] is not None and  # Has text to work with
        scan['extracted_expiry_date'] is None  # No date extracted
    )
```

---

## 🚀 Quick Start Command

```bash
# Clone the integration script
curl -o scanner_orchestrator.py https://your-repo/langchain_integration.py

# Set up environment
cp .env.example .env
# Edit .env with your actual keys

# Run first test
python scanner_orchestrator.py --test-mode

# Expected output:
# 🔄 LangChain Scanner Orchestrator starting...
# 📋 Found 6 scans needing review
# ✅ Enhanced scan 123: 2025-12-25 (confidence: 0.92)
# 🎉 Processing complete: 4 improved, 0 failed
```

---

## 🔍 Next Steps After Phase 1

### Phase 2: Advanced Orchestration
- Multi-agent workflows (Claude for parsing, GPT for validation)
- Intelligent product categorization
- Shopping list automation
- Inventory management integration

### Phase 3: Learning System
- User feedback loop integration
- Continuous improvement based on corrections
- Custom model fine-tuning for your specific products
- Predictive expiration alerts

---

## 📁 Project Structure

```
langchain_scanner/
├── scanner_orchestrator.py     # Main LangChain script
├── tools/
│   ├── ocr_enhancement.py      # GPT-4 OCR tool
│   ├── supabase_connector.py   # Database interface
│   └── nutritionix_tool.py     # Product lookup
├── config/
│   ├── .env                    # Environment variables
│   └── agent_prompts.yaml      # LangChain prompts
├── logs/
│   └── enhancement_log.json    # Processing history
└── tests/
    ├── test_ocr_enhancement.py # Unit tests
    └── test_integration.py     # Integration tests
```

---

## 🎉 Expected Outcomes

**Week 1:** LangChain successfully enhances 3 test scans  
**Week 2:** OCR accuracy improves from 71.5% to 80%+  
**Week 3:** System processes scans automatically with minimal manual review  

**Long-term:** Your scanner becomes intelligent, learning from each correction and automatically handling edge cases that previously required manual intervention.

---

**Ready to start Phase 1?** The foundation is solid, and LangChain will make your existing system significantly more intelligent without disrupting what already works.

Perplexity round 2:
**Review of LangChain Integration v2: Best Practices, Recommendations, and Key Questions**

## ✅ What You’re Doing Well

- **Preserving Existing Investment:** You’re wrapping your current system (iPhone app, Supabase, Claude OCR, Nutritionix, Edge Functions) with LangChain rather than rewriting it. This is a best practice for enterprise AI adoption and minimizes disruption[1][2][3][4].
- **Clear Middleware Role for LangChain:** LangChain is positioned as a middleware/orchestrator for fallback, retries, agent routing, and logging—without replacing your core business logic or data structures[1][4].
- **Phased, Non-Disruptive Rollout:** The integration plan is incremental (environment setup, core script, integration points), with a focus on testing and validation before full rollout[1].
- **Metrics-Driven:** You’re tracking OCR confidence, review rates, and aiming for measurable improvements (e.g., 71.5% → 85% confidence, <10% manual review)[1].
- **Modular Project Structure:** The project is organized with clear separation between orchestrator, tools, config, logs, and tests—this supports maintainability and scaling[1][2][3][5].
- **Error Handling and Fallbacks:** LangChain is set to trigger ChatGPT only when Claude/OCR fails, with logging and error recovery, which is a recommended pattern for multi-agent systems[1][4].

## 🏆 Best Practices Confirmed (and Where You’re On Track)

| Best Practice                    | Your v2 Status                                            | Source(s)         |
|----------------------------------|-----------------------------------------------------------|-------------------|
| **Modular, Reusable Components** | Yes: tools/, config/, logs/, tests/                       | [1][2][3][5]      |
| **Robust Error Handling**        | Yes: fallback to ChatGPT, logging, retry logic            | [1][2][3][5]      |
| **Memory/Context**               | Planned: learning from corrections, context in workflow   | [1][6][4]         |
| **Metrics & Monitoring**         | Yes: OCR stats, logs, success metrics                     | [1][2][7][5]      |
| **Feedback Loop**                | Planned: user corrections, agent learning                 | [1][7]            |
| **Minimal Disruption**           | Yes: no rewriting, only wrapping                          | [1][4]            |
| **Phased Rollout**               | Yes: week-by-week plan, test before production            | [1]               |

## 🛠️ Recommendations to Further Align with Best Practices

1. **Custom Embeddings for Domain-Specific Tasks**
   - If you want to further improve OCR extraction or product lookup, consider training custom embeddings on your own scan data to boost retrieval and accuracy[2][3][5].

2. **Granular Error Handling**
   - Ensure all LangChain calls (including agent handoffs and database updates) are wrapped in try/except blocks with meaningful error messages and fallback logic[2][3][5].

3. **Curate Diverse Test Cases**
   - Use your real scan data and edge cases to build a robust test set for both OCR and fallback flows. This will help you catch rare but critical errors before production[3][5].

4. **Enable and Review Debug Logs**
   - Use LangChain’s `set_debug()` or equivalent logging to monitor chain execution, agent decisions, and bottlenecks. Regular log review is essential for continuous improvement[2][5].

5. **Implement Feedback Loops**
   - Build mechanisms for users (or downstream agents) to flag or correct errors, and feed these corrections back into your system for learning and prompt refinement[7][1].

6. **Memory Strategy**
   - Choose the right memory type for your use case: buffer memory for short tasks, summary or vector memory for long-term context. This will help agents recall corrections and user preferences over time[6][4].

7. **Scalable Agent Communication**
   - As you expand, consider whether you want parallel, sequential, or hierarchical agent flows. For now, your sequential fallback approach is optimal, but design the orchestrator to support more complex flows if needed[6][4].

8. **Evaluator Agents**
   - Integrate evaluator agents to automatically score or flag outputs for review, especially as your workflow grows in complexity[4].

## ❓ Key Questions for You

1. **Schema Details:**  
   - Are there any fields in your Supabase tables that store user feedback or manual corrections? If not, do you want to add one to support feedback loops?

2. **Edge Function Triggers:**  
   - Should LangChain trigger Edge Functions only on failure, or do you want it to also monitor for specific business rules (e.g., product recall, allergy alerts)?

3. **Long-Term Memory:**  
   - Is there a need for persistent memory (e.g., remembering corrections across sessions/users), or is short-term context enough for now?

4. **User Feedback Integration:**  
   - How do you want to collect and act on user corrections? Manual review, thumbs up/down, or structured correction forms?

5. **Prompt Optimization:**  
   - Are you iterating on prompt templates for agents (Claude, ChatGPT), and do you want to automate prompt refinement based on error logs and feedback?

6. **Monitoring and Alerting:**  
   - Do you want automated alerts for high error rates, failed enhancements, or performance drops?

7. **Testing and CI/CD:**  
   - Will you run automated tests (unit/integration) on the orchestrator before each deployment?

## 📝 Summary Table: v2 vs. Best Practices

| Area                 | v2 Status        | Best Practice Alignment      | Recommendation                |
|----------------------|------------------|-----------------------------|-------------------------------|
| Modularity           | Strong           | ✅                           | Maintain                      |
| Error Handling       | Good             | ✅                           | Add granular try/except       |
| Feedback Loop        | Planned          | ✅                           | Implement user correction flow|
| Memory/Context       | Planned          | ✅                           | Choose memory type            |
| Prompt Optimization  | Partial          | ⚠️                           | Automate based on feedback    |
| Testing              | Good             | ✅                           | Expand with edge cases        |
| Monitoring           | Good             | ✅                           | Add alerting if needed        |
| Custom Embeddings    | Not yet          | ⚠️                           | Consider for OCR/lookup       |
| Evaluator Agents     | Not yet          | ⚠️                           | Add for auto QA               |

**You’re on a strong path. The integration plan is modern, modular, and feedback-driven. Focus next on feedback loop implementation, error handling, and memory strategy. Please answer the questions above to further tailor best practices to your workflow.**

Sources
[1] LangChain-Integration-v2.txt https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/70812197/fd7a18e5-5bb5-488d-8229-91be291d3a9c/LangChain-Integration-v2.txt
[2] Unlocking the Power of LangChain: Integration Best Practices for ... https://www.indapoint.com/blog/unlocking-the-power-of-langchain-integration-best-practices-for-modern-ai-applications.html
[3] 5 Best Practices (& Tips) for Using & Integrating LangChain ... - Skim AI https://skimai.com/5-best-practices-tips-for-using-integrating-langchain-for-enterprise-ai/
[4] LangChain & Multi-Agent AI in 2025: Framework, Tools & Use Cases https://blogs.infoservices.com/artificial-intelligence/langchain-multi-agent-ai-framework-2025/
[5] Skim AI Technologies on LinkedIn: 5 Best Practices (& Tips) for Using & Integrating LangChain for Enterprise… https://www.linkedin.com/posts/skim-ai_5-best-practices-tips-for-using-integrating-activity-7204224863811563523-Z9TP
[6] Building Multi AI Agent Workflows With LangChain In 2025 - Intuz https://www.intuz.com/blog/building-multi-ai-agent-workflows-with-langchain
[7] From Fine-Tuning to Feedback Loops: Building Continuous Improvement into LLMOps https://dev.to/rajarshi_tarafdar/from-fine-tuning-to-feedback-loops-building-continuous-improvement-into-llmops-476e
[8] Conceptual guide - Python LangChain https://python.langchain.com/docs/concepts/
[9] Contribute Integrations - Python LangChain https://python.langchain.com/docs/contributing/how_to/integrations/
[10] How to implement an integration package | 🦜️🔗 LangChain https://python.langchain.com/docs/contributing/how_to/integrations/package/
[11] What are some best practices for optimizing LangChain performance? - Zilliz Vector Database https://zilliz.com/ai-faq/what-are-some-best-practices-for-optimizing-langchain-performance
[12] langchain/docs/docs/contributing/how_to/integrations/package.mdx at master · langchain-ai/langchain https://github.com/langchain-ai/langchain/blob/master/docs/docs/contributing/how_to/integrations/package.mdx
[13] Building Feedback Loops in RAG for Continuous Learning https://blog.devgenius.io/building-feedback-loops-in-rag-for-continuous-learning-ecc0c129a15d
[14] Step-by-Step Guide to LangChain Integration - PingCAP https://www.pingcap.com/article/step-by-step-guide-to-langchain-integration/
[15] LangGraph Multi-Agent Systems - Overview https://langchain-ai.github.io/langgraph/concepts/multi_agent/
[16] How to create a custom LLM class - Python LangChain https://python.langchain.com/docs/how_to/custom_llm/
[17] Real-time Automated Feedback | 🦜️🛠️ LangSmith https://docs.smith.langchain.com/old/cookbook/feedback-examples/realtime-algorithmic-feedback
[18] Optimizing LangChain Agent Integration in My Chatbot Pipeline https://www.reddit.com/r/LangChain/comments/1iic9xt/optimizing_langchain_agent_integration_in_my/
[19] Advice needed on orchestrating a multi-agent conversational AI with ... https://www.reddit.com/r/LangChain/comments/1c0xi7a/advice_needed_on_orchestrating_a_multiagent/
[20] Can LangChain integrate with existing ML models or frameworks? https://milvus.io/ai-quick-reference/can-langchain-integrate-with-existing-ml-models-or-frameworks
[21] How to Enable Feedback-Driven Workflow Improvement in Agentic ... https://www.reddit.com/r/LangChain/comments/1hsjdto/how_to_enable_feedbackdriven_workflow_improvement/
Based on your answers and the best practices in the latest research and industry guidance, here are **targeted recommendations and clarifying questions** to help you optimize v2 of your LangChain-based system:

## 1. **Schema for User Feedback/Corrections**
- **Recommendation:**  
  Add a dedicated `feedback` or `review` table in Supabase to store each user’s feedback per scan, including:
  - `scan_id`
  - `user_id` (if applicable)
  - `feedback_type` (thumbs up/down)
  - `feedback_reason` (optional, for thumbs down)
  - `timestamp`  
  This enables long-term tracking, supports analytics, and allows you to link corrections directly to scan events[1][2][3].

## 2. **Edge Function Triggers**
- **Recommendation:**  
  Yes, use LangChain to trigger Edge Functions not only on failure but also for business logic events (e.g., product recalls, allergy alerts, or other compliance needs). This allows for proactive, rule-based automation and keeps your system flexible for future requirements[4].

## 3. **Long-Term Memory**
- **Recommendation:**  
  Use LangChain’s vector or summary memory modules to persist user corrections, feedback, and scan history. Store these in Supabase or a vector database, and have LangChain reference this memory for every new scan or feedback event. This supports longitudinal tracking and enables learning from past corrections, which is best practice for agentic AI systems[5].

## 4. **User Feedback Collection: Thumbs Up/Down with Follow-Up**
- **Recommendation:**  
  - Use a **thumbs up/thumbs down** UI for quick feedback after each scan result is presented[2][6].
  - If the user selects **thumbs down**, prompt a single, optional follow-up question:  
    - “What didn’t work?” or “Can you tell us what was wrong with this result?”  
    - Allow free text, but keep it optional to avoid friction[2][7].
  - Store both the vote and any follow-up in your feedback table.
  - Time the feedback request for after the user completes the scan/review, not before or during the workflow[6].

## 5. **Prompt Optimization**
- **Recommendation:**  
  - Use an **iterative, feedback-driven approach**:  
    - Regularly collect user feedback (especially on thumbs down/corrections).
    - Use this feedback to refine and test prompt variations, ideally with A/B testing or preference-based optimization (see the APOHF method)[8][9].
  - Consider tools or frameworks (like PromptWizard or LangChain’s prompt management) to automate prompt tuning using real user data.
  - Schedule regular prompt review sessions (weekly or bi-weekly) to analyze feedback and adjust prompts accordingly[10][11][9].

## 6. **Monitoring and Alerting**
- **Recommendation:**  
  - Set up real-time monitoring of key metrics: OCR accuracy, agent fallback rates, latency, user satisfaction (thumbs up %), and error rates[12][13][14][15].
  - Use dashboards (Grafana, Supabase UI, or similar) for visibility.
  - Configure automated alerts (Slack, email, etc.) for:
    - Performance drops (accuracy, latency, fallback rate)
    - High rates of negative feedback or corrections
    - System errors or failed Edge Functions[12][13][15].
  - Integrate alerting into your CI/CD pipeline to block deploys on active alerts[13].

## 7. **Automated Testing and CI/CD**
- **Recommendation:**  
  - Build automated tests for each workflow step (scan, OCR, agent handoff, feedback logging).
  - Integrate these tests into your CI/CD pipeline so every deploy runs a full suite, catching regressions early[16][17].
  - Include edge cases and real-world scan data in your test set.
  - Use auto-QA agents or scripts to score outputs and flag anomalies for review[18][17][19][20].

## **Open Questions for You**

1. **User Identification:**  
   - Will feedback be anonymous, or do you want to link it to specific users for personalized improvement and memory?
2. **Feedback Volume:**  
   - Do you expect high user engagement with feedback, or will it be sporadic? This affects how aggressively you prompt for feedback and how you prioritize follow-up.
3. **Correction Application:**  
   - Should corrections automatically retrain or adjust agent prompts, or do you want a manual review step before changes are applied?
4. **Alert Escalation:**  
   - Who should receive alerts for performance drops or high error rates? Is there an on-call rotation, or should alerts go to a group/team?

## **Summary Table: Your v2 Plan vs. Best Practices**

| Area                   | Your Plan         | Best Practice Recommendation          |
|------------------------|-------------------|---------------------------------------|
| Feedback Storage       | Not yet in schema | Add feedback/review table in Supabase |
| Edge Function Triggers | Yes               | Use for both failures & business logic|
| Long-Term Memory       | Yes               | Use LangChain vector/summary memory   |
| Feedback UI            | Thumbs up/down    | Add optional follow-up for thumbs down|
| Prompt Optimization    | Not defined       | Use feedback-driven, iterative tuning |
| Monitoring/Alerting    | Yes               | Real-time, dashboards, CI/CD blocking |
| Automated Testing      | Want to           | Integrate into CI/CD, cover edge cases|

**You’re on track with industry best practices. Implementing these recommendations will give you a robust, feedback-driven, and continuously improving system. Let me know your preferences on the open questions above to further tailor the plan.**

Sources
[1] How to store feedback like stars or votes of users with efficiency? https://stackoverflow.com/questions/54571991/how-to-store-feedback-like-stars-or-votes-of-users-with-efficiency
[2] Collecting Feedback with Thumbs Up/Down Surveys https://www.zonkafeedback.com/blog/collecting-feedback-with-thumbs-up-thumbs-down-survey
[3] Obtain feedback for every response - Microsoft Copilot Studio https://learn.microsoft.com/en-us/microsoft-copilot-studio/guidance/adaptive-card-add-feedback-for-every-response
[4] MindFlow-Rise-of-the-Machines-PRD-v1.txt https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/70812197/3c817925-518b-4027-984b-a40f14ad847b/MindFlow-Rise-of-the-Machines-PRD-v1.txt
[5] AI Agent Evaluation: Building Trustworthy and Autonomous Systems https://www.cmarix.com/blog/ai-agent-evaluation/
[6] User-Feedback Requests: 5 Guidelines - NN/g https://www.nngroup.com/articles/user-feedback/
[7] Follow-Up Questions - SmartSurvey https://www.smartsurvey.com/survey-questions/follow-up-questions
[8] [2405.17346] Prompt Optimization with Human Feedback - arXiv https://arxiv.org/abs/2405.17346
[9] PromptWizard: The future of prompt optimization through feedback ... https://www.microsoft.com/en-us/research/blog/promptwizard-the-future-of-prompt-optimization-through-feedback-driven-self-evolving-prompts/
[10] 7 Best Tips for Gathering Feedback on Prompts - Jonathan Mast https://jonathanmast.com/7-best-tips-for-gathering-feedback-on-prompts/
[11] Using User Feedback to Improve AI Prompt Performance https://whitebeardstrategies.com/blog/using-user-feedback-to-improve-ai-prompt-performance/
[12] AI Monitoring Explained: Keep Models Accurate and Reliable - Cribl https://cribl.io/glossary/ai-monitoring/
[13] Setting up alerts for performance degradation - Mue AI https://muegenai.com/docs/gen-ai/mlops-deploying-managing-machine-learning-in-production/module-6-model-monitoring-logging/setting-up-alerts-for-performance-degradation/
[14] AI Agent Monitoring: Essential Metrics and Best Practices https://ardor.cloud/blog/ai-agent-monitoring-essential-metrics-and-best-practices
[15] AI Monitoring: Strategies, Tools & Real-World Use Cases https://uptimerobot.com/knowledge-hub/monitoring/ai-monitoring-guide/
[16] AI agent evaluation: Metrics, strategies, and best practices - Wandb https://wandb.ai/onlineinference/genai-research/reports/AI-agent-evaluation-Metrics-strategies-and-best-practices--VmlldzoxMjM0NjQzMQ
[17] Quality Assurance Manager AI Agent - Akira AI https://www.akira.ai/ai-agents/quality-assurance-manager-ai-agent
[18] Auto QA | Conversation Analytics - EvaluAgent https://www.evaluagent.com/platform/auto-qa/
[19] AUTO QA for Contact Centers - Observe.AI https://www.observe.ai/post-interaction/auto-qa
[20] Automate agent quality management with AI | Sprinklr https://www.sprinklr.com/blog/automate-agent-quality-management-with-AI/
[21] Follow-Up Surveys: Questions and Examples | SurveyPlanet https://blog.surveyplanet.com/follow-up-surveys-when-and-how-to-ask-additional-questions
[22] Automated Agent Evaluations – Soho2 - Sohosquared https://sohosquared.com/solutions/compliance-assessment-services/
[23] AI Error Tracker Ensuring Reliable AI Operations with Structured Monitoring https://www.youtube.com/watch?v=_DtDT5qiDxk
[24] QA Automation for Contact Centers - AI-powered - Creovai https://www.creovai.com/platform/qa-automation
[25] AI Testing Agents Explained: Automating QA for Maximum Efficiency https://www.accelirate.com/ai-testing-agents/
[26] User Management | Supabase Docs https://supabase.com/docs/guides/auth/managing-user-data
[27] Visual Schema Designer | Supabase Features https://supabase.com/features/visual-schema-designer
[28] Restricting Access on Auth, Storage, and Realtime Schemas on ... https://github.com/orgs/supabase/discussions/34270
[29] Programmatically create new users #5043 - supabase - GitHub https://github.com/orgs/supabase/discussions/5043
[30] Local development with schema migrations | Supabase Docs https://supabase.com/docs/guides/local-development/overview
[31] EXPLORING MANUAL CORRECTION AS A SOURCE OF USER ... https://research.manchester.ac.uk/en/studentTheses/exploring-manual-correction-as-a-source-of-user-feedback-in-pay-a
[32] r/Supabase on Reddit: What were the top mistakes you made when ... https://www.reddit.com/r/Supabase/comments/1f41a3v/what_were_the_top_mistakes_you_made_when_starting/
[33] How to Store User Feedback in MongoDB for Your Event App https://www.youtube.com/watch?v=00mczB5eb68
[34] Database schema design for posts, comments and replies https://stackoverflow.com/questions/37456154/database-schema-design-for-posts-comments-and-replies
[35] Declarative Schemas for Simpler Database Management - Supabase https://supabase.com/blog/declarative-schemas
[36] Database Design for Customer Reviews and Ratings Platform https://www.geeksforgeeks.org/how-to-design-a-relational-database-for-customer-reviews-and-ratings-platform/
[37] Database schema design implementation feedback https://dba.stackexchange.com/questions/273233/database-schema-design-implementation-feedback
[38] How Supabase Simplifies Your Database Management ... - YouTube https://www.youtube.com/watch?v=EALkUlOKvAs&vl=en
[39] What are the best practices for keeping and analyzing unstructured ... https://ux.stackexchange.com/questions/94013/what-are-the-best-practices-for-keeping-and-analyzing-unstructured-user-feedback
[40] Database design basics - Microsoft Support https://support.microsoft.com/en-us/office/database-design-basics-eb2159cf-1e30-401a-8084-bd4f9c9ca1f5
[41] [HELP] Could not find a relationship between 'profiles' and ... - Reddit https://www.reddit.com/r/Supabase/comments/13l4rqf/help_could_not_find_a_relationship_between/
[42] Storing user customisations and settings. How do you do it? https://dev.to/imthedeveloper/storing-user-customisations-and-settings-how-do-you-do-it-1017
[43] Building a Relational Database: Complete Guide - Knack https://www.knack.com/blog/how-to-design-an-effective-relational-database/
[44] How to access custom schema from supabase-js client? https://stackoverflow.com/questions/73281996/how-to-access-custom-schema-from-supabase-js-client
[45] How to position Thumbs Up / Thumbs Down with progression on ... https://ux.stackexchange.com/questions/98733/how-to-position-thumbs-up-thumbs-down-with-progression-on-one-line
[46] Thumbs up/down survey | SnapSurveys https://www.snapsurveys.com/marketplace/surveys-and-assessments/thumbs-up-down-survey/
[47] Most effective way to use thumbs for feedback? : r/NomiAI - Reddit https://www.reddit.com/r/NomiAI/comments/140wk3b/most_effective_way_to_use_thumbs_for_feedback/
[48] Collecting Customer Feedback with Thumbs Survey https://melroseapp.com/blog/collect-feedback-with-thumbs-survey/
[49] 5 stars vs. thumbs up/down—which rating system is right for your app? https://www.appcues.com/blog/rating-system-ux-star-thumbs
[50] Content usage guideline violation - Thumbs up/down feedback https://community.openai.com/t/content-usage-guideline-violation-thumbs-up-down-feedback/761940
[51] Thumbs Projects - Help Center - Delighted https://delighted-help-160b834b9adfde78ebef1528.helpscoutdocs.com/article/536-thumbs-surveys
[52] Phase 5: Checking the effectiveness of measures & follow-up survey https://easy-feedback.com/blog/feedback-loop-explained/checking-the-effectiveness-of-measures-follow-up-survey/
[53] Solved: Adding thumbs-up or thumbs-up to page or learning https://community.canvaslms.com/t5/Question-Forum/Adding-thumbs-up-or-thumbs-up-to-page-or-learning-object-to/m-p/469277
[54] Suggestion for Improving Thumbs-Up/Thumbs-Down Feedback ... https://community.openai.com/t/suggestion-for-improving-thumbs-up-thumbs-down-feedback-system/1228407
[55] How to Create a Feedback Loop: Step-By-Step Guide + Best Practices https://userpilot.com/blog/how-to-create-a-feedback-loop/
[56] Implementing a feedback mechanism in your chatbot applications https://developer.ibm.com/tutorials/awb-watsonx-assistant-thumbs-up-down-feedback/
[57] What are typical response rates for insite surveys and thumbs up ... https://www.reddit.com/r/UXResearch/comments/15zacwn/what_are_typical_response_rates_for_insite/
[58] System Implementation Feedback Survey | 50+ Key Questions https://www.poll-maker.com/system-implementation-feedback
[59] Recommendation Engine - handling thumbs up/down - Microsoft Q&A https://learn.microsoft.com/en-us/answers/questions/1689489/recommendation-engine-handling-thumbs-up-down
[60] Optimize query responses with user feedback using Amazon ... https://aws.amazon.com/blogs/machine-learning/optimize-query-responses-with-user-feedback-using-amazon-bedrock-embedding-and-few-shot-prompting/
[61] Automated prompt optimisation : r/PromptEngineering - Reddit https://www.reddit.com/r/PromptEngineering/comments/1fhe3g4/automated_prompt_optimisation/
[62] Prompt Optimization Using Few-Shot Prompting - Arize AI https://arize.com/blog/prompt-optimization-few-shot-prompting/
[63] Mastering Prompt Tuning for AI Efficiency https://startup-house.com/blog/guide-to-prompt-tuning
[64] Prompt Optimization: A Comprehensive Guide (2025) - Orq.ai https://orq.ai/blog/prompt-optimization
[65] Log Analysis and Fix Suggestion - AI Prompt https://docsbot.ai/prompts/technical/log-analysis-and-fix-suggestion
[66] Future of Feedback Driven Prompt Optimization - Advances in AI https://www.linkedin.com/pulse/future-feedback-driven-prompt-optimization-advances-shakrapani-sm2bc
[67] Using AI to mine log messages from reference doc builds https://idratherbewriting.com/ai/prompt-engineering-mine-log-messages.html
[68] Prompt Tuning vs. Fine-Tuning—Differences, Best Practices ... - Nexla https://nexla.com/ai-infrastructure/prompt-tuning-vs-fine-tuning/
[69] Prompt Optimization with Human Feedback - arXiv https://arxiv.org/html/2405.17346v1
[70] Automated-Prompt-Engineering-Hub/README.md at main · The-Swarm-Corporation/Automated-Prompt-Engineering-Hub https://github.com/The-Swarm-Corporation/Automated-Prompt-Engineering-Hub/blob/main/README.md
[71] Prompt engineering best practices: Top 10 tips https://www.hostinger.com/tutorials/prompt-engineering-best-practices
[72] Automating Prompt Optimization: A Free Tool to Fix ChatGPT ... https://www.reddit.com/r/PromptEngineering/comments/1igjhjs/automating_prompt_optimization_a_free_tool_to_fix/
[73] Automatic Prompt Refinement for Enhancing Code Generation - arXiv https://arxiv.org/html/2503.11085v1
[74] Any resource guides for prompt tuning/writing : r/PromptEngineering https://www.reddit.com/r/PromptEngineering/comments/1j7oxs8/any_resource_guides_for_prompt_tuningwriting/
[75] Overview of prompting strategies | Generative AI on Vertex AI | Google Cloud https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies
[76] Understanding AI Observability: Key Insights for Effective Monitoring https://www.signitysolutions.com/blog/understanding-ai-observability
[77] Alerting on SLOs https://sre.google/workbook/alerting-on-slos/
[78] Monitoring performance of clinical artificial intelligence in health care https://pmc.ncbi.nlm.nih.gov/articles/PMC11630661/
[79] Pattern 2D: Provide low performance alerts - Microsoft HAX Toolkit https://www.microsoft.com/en-us/haxtoolkit/pattern/g2-d-provide-low-performance-alerts/
[80] How to Implement Real-Time Alerts for API Gateway Failures - API7.ai https://api7.ai/blog/how-to-implement-real-time-alerts
[81] AI Monitoring: Common Issues Solved - Serverion https://www.serverion.com/uncategorized/ai-monitoring-common-issues-solved/
[82] Does Your AI Monitoring System Follow These Best Practices? https://www.lumenova.ai/blog/ai-monitoring-system-best-practices/
[83] Set up Cloud Monitoring alerts | Vertex AI Search for commerce https://cloud.google.com/retail/docs/monitor
[84] Performance monitoring and alerts for gen AI models on Vertex AI https://cloud.google.com/blog/products/ai-machine-learning/performance-monitoring-and-alerts-for-gen-ai-models-on-vertex-ai
[85] Best Practices for Monitoring and Logging in AI Systems https://magnimindacademy.com/blog/best-practices-for-monitoring-and-logging-in-ai-systems/
[86] 10-Step AIOps Alert Management Guide 2024 - Eyer.ai https://www.eyer.ai/blog/10-step-aiops-alert-management-guide-2024/
[87] Solved: How to configure application HTTP error rate alerts? https://community.dynatrace.com/t5/Alerting/How-to-configure-application-HTTP-error-rate-alerts/m-p/162878
[88] AI Model Drift Uncovered: Strategies to Keep Your Models Accurate ... https://miamifed.com/ai-model-drift/
[89] AI Scheduling API Error Protocols: Integration Success Blueprint https://www.myshyft.com/blog/error-handling-protocols/
[90] AI in observability: Advancing system monitoring and performance https://newrelic.com/blog/how-to-relic/ai-in-observability
[91] AI-Powered Automated Quality Management - Verint https://www.verint.com/automated-quality-management/
[92] Product Quality Assurance | AI Agents & Agentic Worfklows https://beam.ai/workflows/product-quality-assurance
[93] AI Agent Evaluation | UiPath https://www.uipath.com/platform/agentic-automation/agent-evaluation
[94] AI-Powered Quality Agents Transform Data Management: Innovation at Scale https://www.analyticsinsight.net/artificial-intelligence/ai-powered-quality-agents-transform-data-management-innovation-at-scale
[95] AI Agent Evaluation: How to Conduct Effectively? - Markovate https://markovate.com/ai-agent-evaluation/
[96] Agent System for Auto Evaluating Data - Galileo AI https://galileo.ai/blog/building-agent-system-for-auto-evaluating-data
[97] Best Practices to Navigate the Complexities of Evaluating AI Agents https://galileo.ai/blog/evaluating-ai-agents-best-practices
[98] Call Center QA & Performance Improvement Software - evaluagent https://www.evaluagent.com
[99] AI Agents in QA Testing: Is 2025 The Year Everything Changes? https://momentic.ai/blog/ai-agents-in-qa-testing
[100] Agent Evaluation in 2025: Complete Guide - Orq.ai https://orq.ai/blog/agent-evaluation
[101] Agent Evaluation for AI Performance and Automation - NexaStack https://www.nexastack.ai/ai-agents/agentevaluation/


Gemini v2:

Based on the comprehensive "LangChain Integration v2" plan, it's clear there is a strong, unified strategy in place. The documents show an excellent synthesis of the previous feedback and a clear commitment to a non-disruptive, value-add integration. Here is a critical review of the v2 plan, highlighting its alignment with best practices, offering specific recommendations for implementation, and asking key questions to guide the next phase of development.

Overall Assessment

The v2 integration plan is a superb example of how to correctly layer a powerful framework like LangChain onto an existing, operational system. The "wrapper" philosophy is the right approach, as it preserves months of work while using AI to solve a specific, measurable business problem (improving low-confidence OCR results). The alignment between ChatGPT's high-level mission and Claude's detailed, phased implementation plan is excellent.

Best Practices in Your v2 Plan

Your current plan already incorporates several industry best practices:

Non-Disruptive Integration: The core principle is to wrap the existing system, not rewrite it. This minimizes risk and leverages your current, working components.




Phased and Measured Rollout: The plan is broken down into a clear, week-by-week checklist, starting with foundational setup and testing before moving to more complex agentic logic.



Metrics-Driven Approach: Success is tied to concrete, measurable outcomes, such as improving the average OCR confidence from 71.5% to over 85% and reducing the need for manual review.



Modular Project Structure: The proposed folder structure cleanly separates the core orchestrator from tools, configuration, logs, and tests, which is essential for long-term maintainability.



Intelligent Fallback Logic: The system is designed to be a "background processor" that only activates when needed, using LangChain to enhance specific scans where the initial OCR confidence is low.



Actionable Recommendations

To further enhance the v2 plan and ensure its successful implementation, consider these specific recommendations:

Formalize Prompt Management:

Problem: The enhance_ocr_with_gpt4 function hardcodes the prompt as a Python f-string. This makes it difficult to version, test, and manage prompts without changing the application code.

Recommendation: Use the agent_prompts.yaml file proposed in your project structure. Load the prompts from this external configuration file at runtime. This separates your prompts (which are a form of configuration) from your logic, allowing for easier iteration and A/B testing of different prompt templates.


Implement Granular Error Handling:

Problem: A failure during the enhancement process can be ambiguous. Did the LLM fail to respond correctly, or did the final database update fail?

Recommendation: Wrap the llm.predict() call and the supabase.table().update() call in separate try/except blocks. This allows you to log the specific point of failure. For example, you could update the scan's status to 

"enhancement_failed" or "db_update_failed", providing more precise data for debugging.

Define a Specific Memory Strategy:

Problem: The plan mentions adding "memory for learning from corrections," which is excellent. However, the type of memory is crucial for its effectiveness.


Recommendation: For the goal of learning from corrections over time, plan to use a form of persistent memory like LangChain’s VectorStoreRetrieverMemory. You can store user corrections in a dedicated Supabase table  and use this data to create embeddings. For each new low-confidence scan, the agent can retrieve examples of similar, previously corrected scans to improve its accuracy. This is more powerful than simple buffer memory for long-term improvement.



Build a Curated Test Suite:

Problem: Testing needs to cover a wide range of real-world scenarios.

Recommendation: Create a dedicated test_data.json file. Populate it with examples of scans from your database that represent key edge cases: scans with very blurry OCR text, scans with unusual date formats, and scans where the initial OCR was correct but confidence was low. Your unit and integration tests should load this file and assert that the orchestrator behaves as expected for each case.




Key Questions for Your Next Iteration

Your answers to these questions will shape the evolution from a simple fallback processor to a truly intelligent, multi-agent system.

Correction Application Workflow: When a user manually corrects an expiration date that LangChain enhanced, what should happen? Should that correction automatically be used to fine-tune future prompts, or should it be flagged for a manual review process before being incorporated into the system's memory?

Agent Chaining Potential: Beyond the current fallback logic, do you envision a more complex chain of agentic steps? For example, if OCR fails entirely, should the agent first use a tool to look up the product on Nutritionix, find common packaging styles for that product, and then re-prompt the OCR enhancement tool with that new context?

Cost vs. Performance Tiering: The plan specifies using "gpt-4" for enhancement. Have you considered a tiered approach? For example, using a faster, cheaper model for scans with moderate confidence (e.g., 60-80%) and only escalating to the more powerful GPT-4 for the most difficult cases (<60% confidence)?

Defining "Manual Review": Who receives the alerts for scans that still need review after the LangChain enhancement fails or produces a low-confidence result? Is there a specific user or team, and what interface will they use to perform this final review?

1. automatic. 2. yes. 3. no issues yet but if we start running into problems this should be a fallback. 4. I am a team of one. I need an interface and alerts.
Based on your answers, here is a set of targeted recommendations to build out your LangChain integration, focusing on creating an automated feedback loop and a practical review interface for a team of one.

Action Plan: Building Your Review and Learning Loop

Your direction is clear: you want an automated system that learns from your corrections and a simple, effective way to handle the scans that require manual review. Here's how to build that.

1. Implement the Manual Review System

This is your most immediate need. A robust review system has two parts: the alert that tells you something needs attention, and the interface where you can take action.

Alerting

Since you're a team of one, the alerting system should be simple and direct. When your LangChain script fails to enhance a scan and leaves its status as needs_review, it should trigger an alert.

Recommendation: Use a simple email alert directly from your Python script.

How: Integrate a service like SendGrid or Resend. They have free tiers and simple Python clients.

Logic: In your scanner_orchestrator.py script, after the loop that processes the scans, check if any scans still have the needs_review status. If so, call the email function to send a notification to your address with a subject like "Scanner Review Required" and a body listing the IDs of the scans needing review.

Interface

You need a straightforward UI to view problematic scans and enter corrections. You don't need a complex web application; a simple data app is perfect.

Recommendation: Build a simple web interface using Streamlit.

Why: Streamlit lets you build powerful data apps with just Python. It's incredibly fast for creating internal tools.

How to Build It:

Create a new Python file (e.g., review_app.py).

The app will connect to your Supabase database.

It will query the scanned_items table for all records where verification_status is 'needs_review'.

For each item, it will display the product name, the raw OCR text, and an input box for the corrected expiration date.

A "Save" button next to each item will trigger a function to update the Supabase record with the correct date and change the status to 'approved'.

2. Automate the Correction Feedback Loop

You want the system to learn automatically from your manual corrections. This is a perfect use case for a persistent memory strategy in LangChain.

Recommendation: Use Supabase's pgvector extension to create a vector store for your corrections.

Step 1: Create a corrections Table: In Supabase, create a new table to store your manual fixes. It should have columns like id, created_at, scan_id, original_ocr_text, and corrected_date.

Step 2: Update Your Review App: When you save a correction in your Streamlit review app, it should now do two things:

Update the original scanned_items record.

Add a new entry into the corrections table with the details of the fix.

Step 3: Implement VectorStoreRetrieverMemory: In your LangChain orchestrator, use a vector store that reads from your new corrections table. Before processing a new low-confidence scan, the agent will first retrieve examples of similar OCR texts that you've corrected in the past.

Step 4: Enhance the Agent's Prompt: Modify the agent's prompt to use this retrieved information. The prompt will now effectively say:

"You are an expert at reading expiration dates. The following OCR text is from a {product_name}: '{ocr_text}'. For your reference, here are some similar texts and the correct dates that were extracted from them: {retrieved_corrections}. Now, extract the expiration date from the OCR text."

This creates a powerful, self-improving system where every manual correction directly teaches the AI how to handle similar cases in the future.

3. Plan for Advanced Agent Chaining (Phase 2)

Since you're open to more sophisticated workflows, you can plan to implement an advanced agent chain after the review system and feedback loop are operational.

Recommendation: Develop a "Context-Enhanced OCR Chain."

Trigger: This chain would run when the initial OCR text is either missing or has extremely low confidence (e.g., < 20%).

Step-by-Step Logic:

Tool 1 (Nutritionix): The agent first calls the NutritionixLookupTool using the barcode to get the product's exact name and brand.

Reasoning Step: The agent now has new context. It knows it's looking at "Campbell's Chicken Noodle Soup" and not just a random string of OCR text.

Tool 2 (OCR Enhancement): The agent calls the OCREnhancementTool again, but this time with a much more powerful, context-aware prompt:

"Given that this product is '{product_name}' by '{brand_name}', extract the expiration date from the following OCR text: '{ocr_text}'."

This multi-step reasoning process significantly increases the chances of a successful extraction on the first pass, further reducing the need for manual review.

P
